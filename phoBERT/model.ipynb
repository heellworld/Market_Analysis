{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import underthesea # Thư viện tách từ\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split # Thư viện chia tách dữ liệu\n",
    "from transformers import AutoModel, AutoTokenizer # Thư viện BERT\n",
    "from underthesea import word_tokenize as word_tokenize_vn\n",
    "# Thư viện train SVM\n",
    "from sklearn.svm import SVC\n",
    "from joblib import dump\n",
    "from underthesea import sentiment\n",
    "import re\n",
    "import string\n",
    "from underthesea import word_tokenize\n",
    "import emoji  # Đảm bảo rằng thư viện này được cài đặt\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import regex\n",
    "import statistics \n",
    "import regex as re\n",
    "from transformers import get_linear_schedule_with_warmup, AutoTokenizer, AutoModel, logging\n",
    "from underthesea import word_tokenize\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from joblib import dump\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import emoji  # Đảm bảo rằng thư viện này được cài đặt\n",
    "from datetime import datetime, timedelta\n",
    "import nltk\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import statistics \n",
    "import regex as re\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(86)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a maximum of length of 10 is safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = 'Comment_segement.csv'\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "def load_bert():\n",
    "    model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
    "    return model, tokenizer\n",
    "\n",
    "stopwords_file = r'D:/projects/NCKH/Data_Processing/vietnamese-stopwords.txt'\n",
    "lookup_dict_file = r\"D:\\projects\\NCKH\\Data_Processing\\lookup_dict.txt\"\n",
    "\n",
    "def load_lookup_dictionary(lookup_dict_file):\n",
    "    lookup_dict = {}\n",
    "    with open(lookup_dict_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)  # Phân tách chỉ tại dấu hai chấm đầu tiên\n",
    "                key = key.strip()  # Loại bỏ khoảng trắng thừa ở khoá\n",
    "                value = value.strip()  # Loại bỏ khoảng trắng thừa ở giá trị\n",
    "                lookup_dict[key] = value\n",
    "    return lookup_dict\n",
    "lookup_dict = load_lookup_dictionary(lookup_dict_file)\n",
    "\n",
    "def load_stopwords(filepath=\"vietnamese-stopwords.txt\"):\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        stopwords = [line.strip() for line in f.readlines()]\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "def remove_stopwords(text, stopwords):\n",
    "    # Tách từ và loại bỏ stopwords\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stopwords]\n",
    "    # Nối các từ đã lọc thành một chuỗi mới\n",
    "    filtered_text = ' '.join(filtered_words)\n",
    "    return filtered_text\n",
    "stopwords = load_stopwords(stopwords_file)\n",
    "\n",
    "def remove_non_words(text):\n",
    "    words = text.split()\n",
    "    valid_words = [\n",
    "        word for word in words \n",
    "        if 2 <= len(word) <= 8\n",
    "        and not re.search(r'(.)\\1{3,}', word)\n",
    "        and len(set(word)) < 8 \n",
    "    ]\n",
    "    filtered_text = ' '.join(valid_words)\n",
    "    return filtered_text\n",
    "\n",
    "def split_sentences(text):\n",
    "    # Xóa các ký tự xuống dòng\n",
    "    cleaned_text = text.replace('\\n', ' ')\n",
    "    # Tách văn bản thành các câu bằng cách sử dụng dấu chấm\n",
    "    sentences = cleaned_text.split('.')\n",
    "    # Loại bỏ chuỗi trống và dư khoảng trắng\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    return sentences\n",
    "\n",
    "def remove_words(text):\n",
    "    # Biểu thức chính quy để xác định từ bắt đầu bằng các ký tự có dấu\n",
    "    pattern = r'\\b[ÁáÀàẢảÃãẠạĂăẮắẰằẲẳẴẵẶặÂâẤấẦầẨẩẪẫẬậÉéÈèẺẻẼẽẸẹÊêẾếỀềỂểỄễỆệÍíÌìỈỉĨĩỊịÓóÒòỎỏÕõỌọÔôỐốỒồỔổỖỗỘộƠơỚớỜờỞởỠỡỢợÚúÙùỦủŨũỤụƯưỨứỪừỬửỮữỰựÝýỲỳỶỷỸỹỴỵ]\\S*'\n",
    "    # Loại bỏ các từ khớp với biểu thức chính quy ở đầu câu\n",
    "    text = re.sub(pattern, '', text, count=1)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text, lookup_dict):\n",
    "    # Tách từng từ trong văn bản để kiểm tra từng từ có trong lookup_dict không\n",
    "    words = text.split()\n",
    "    # Thay thế từng từ nếu nó xuất hiện trong từ điển\n",
    "    normalized_words = [lookup_dict[word] if word in lookup_dict else word for word in words]\n",
    "    # Nối lại các từ đã chuẩn hóa thành một chuỗi mới\n",
    "    normalized_text = ' '.join(normalized_words)\n",
    "    return normalized_text\n",
    "\n",
    "def replace_with_dictionary(text, dictionary):\n",
    "    # Sắp xếp các cụm từ từ dài đến ngắn để đảm bảo rằng cụm từ dài nhất được áp dụng trước\n",
    "    dictionary = sorted(dictionary, key=len, reverse=True)\n",
    "    for phrase in dictionary:\n",
    "        # Tạo mẫu regex để khớp cụm từ với khoảng trắng hoặc gạch dưới giữa các từ\n",
    "        words = phrase.split()\n",
    "        pattern = r'\\b' + r'[_\\s]*'.join([re.escape(word) for word in words]) + r'\\b'\n",
    "        replacement = \"_\".join(words)\n",
    "        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "def standardize_data(row):\n",
    "    row = row.lower()\n",
    "    \n",
    "    # Loại bỏ URLs and HTML\n",
    "    row = re.sub(r'http\\S+|www\\S+|https\\S+|<[^>]+>|\\b(href|a)\\b', '', row, flags=re.MULTILINE)\n",
    "    # Loại bỏ ký tự đặc biệt\n",
    "    row = re.sub(r'[^\\w\\s\\.]', '', row, flags=re.UNICODE)\n",
    "    # Loại bỏ từ lặp lại liên tiếp > 2\n",
    "    row = re.sub(r'(.)\\1{2,}', r'\\1', row)\n",
    "    # Thay thế chuỗi trắng liên tiếp bằng chuỗi trắng đơn\n",
    "    row = re.sub(r'\\s+', ' ', row)\n",
    "\n",
    "    # Loại bỏ những từ xuất hiện < 6 kí tự và >2 kí tự\n",
    "    row = remove_non_words(row)\n",
    "\n",
    "    # Loại bỏ những từ có các nguyên âm đứng đầu câu\n",
    "    row = remove_words(row)\n",
    "\n",
    "    # Loại bỏ các ký tự tiếng Trung\n",
    "    row = regex.sub(r'\\p{Script=Han}+', '', row)\n",
    "    # Loại bỏ các ký tự tiếng Nga (Cyrillic)\n",
    "    row = regex.sub(r'\\p{Script=Cyrillic}+', '', row)\n",
    "    # Loại bỏ các ký tự tiếng Nhật\n",
    "    row = regex.sub(r'\\p{Script=Hiragana}+', '', row)\n",
    "    row = regex.sub(r'\\p{Script=Katakana}+', '', row)\n",
    "    row = regex.sub(r'\\p{IsHan}+', '', row)  # Bổ sung cho Kanji\n",
    "    # Loại bỏ các ký tự tiếng Ấn Độ (Devanagari)\n",
    "    row = regex.sub(r'\\p{Script=Devanagari}+', '', row)\n",
    "    \n",
    "    # Loại bỏ các ký tự toán học in đậm\n",
    "    row = regex.sub(r'\\p{Math}', '', row)\n",
    "    \n",
    "    # Loại bỏ các ký tự không phải là chữ cái hoặc số\n",
    "    row = regex.sub(r'[^\\p{L}\\p{N}\\.]+', ' ', row)\n",
    "\n",
    "    # Chuẩn hóa khoảng trắng\n",
    "    row = re.sub(r'\\s+', ' ', row).strip()\n",
    "\n",
    "    # Convert emojis to row\n",
    "    row = emoji.demojize(row, delimiters=(\"_\", \"_\"))\n",
    "    # Sử dụng regex để tìm và xóa các chuỗi emoji đã được chuẩn hóa\n",
    "    row = re.sub(r'_[a-zA-Z0-9_]+_', '', row)\n",
    "\n",
    "    # Xóa số\n",
    "    row = re.sub(\"\\d+\", \" \", row)\n",
    "\n",
    "    row = normalize_text(row, lookup_dict)\n",
    "    row = remove_stopwords(row, stopwords)\n",
    "    row = re.sub(r'\\bkhông\\b\\s*\\?*\\s*$', '', row)\n",
    "    # Loại bỏ từ trùng lặp\n",
    "    row = re.sub(r'\\b(\\w+)(?:\\s+\\1\\b)+', r'\\1', row)\n",
    "\n",
    "    # Remove digits\n",
    "    row = re.sub(\"\\d+\", \" \", row)\n",
    "    return row.strip().lower()\n",
    "\n",
    "def load_data(filepath=file_path):\n",
    "    df = pd.read_csv(filepath, encoding='utf-8')\n",
    "    df['cleaned'] = df['sentence'].apply(standardize_data)\n",
    "    return df['cleaned'].values, df['sentiment'].values\n",
    "\n",
    "def make_bert_features(texts, tokenizer, model, max_len=20, stopwords=[]):\n",
    "    tokenized = []\n",
    "    for text in texts:\n",
    "        # Loại bỏ stopwords\n",
    "        text = \" \".join([word for word in word_tokenize(text) if word.lower() not in stopwords])\n",
    "        \n",
    "        # Mã hóa câu văn thành token\n",
    "        encoded = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_len, return_tensors=\"pt\", pad_to_max_length=True, return_attention_mask=True, truncation=True)\n",
    "        input_ids = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "        \n",
    "        # Tính toán biểu diễn ngữ cảnh của câu\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Lấy vectơ embedding của token [CLS]\n",
    "        features = last_hidden_states[0][:, 0, :].detach().numpy()\n",
    "        tokenized.append(features)\n",
    "    \n",
    "    # Xếp chồng các vectơ embedding của các câu\n",
    "    return np.vstack(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chuẩn bị nạp danh sách các từ vô nghĩa (stopwords)...\n",
      "Đã nạp xong danh sách các từ vô nghĩa\n",
      "Chuẩn bị nạp model BERT....\n",
      "Đã nạp xong model BERT.\n",
      "Chuẩn bị load dữ liệu....\n",
      "Đã load dữ liệu xong\n",
      "Chuẩn bị tạo features từ BERT.....\n",
      "Đã tạo xong features từ BERT\n"
     ]
    }
   ],
   "source": [
    "# Log messages\n",
    "print(\"Chuẩn bị nạp danh sách các từ vô nghĩa (stopwords)...\")\n",
    "sw = load_stopwords()\n",
    "print(\"Đã nạp xong danh sách các từ vô nghĩa\")\n",
    "\n",
    "print(\"Chuẩn bị nạp model BERT....\")\n",
    "phobert, tokenizer = load_bert()\n",
    "print(\"Đã nạp xong model BERT.\")\n",
    "\n",
    "print(\"Chuẩn bị load dữ liệu....\")\n",
    "text, label = load_data()\n",
    "print(\"Đã load dữ liệu xong\")\n",
    "\n",
    "print(\"Chuẩn bị tạo features từ BERT.....\")\n",
    "features = make_bert_features(text, tokenizer, phobert, stopwords=sw)\n",
    "print(\"Đã tạo xong features từ BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chuẩn bị train model SVM....\n",
      "Kết quả train model, độ chính xác =  94.98607242339833 %\n",
      "Đã lưu model SVM vào file model_phoBERT.pkl.pkl\n"
     ]
    }
   ],
   "source": [
    "# Chia tập dữ liệu\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=86)\n",
    "\n",
    "# Huấn luyện mô hình SVM\n",
    "print(\"Chuẩn bị train model SVM....\")\n",
    "cl = SVC(kernel='linear', probability=True, gamma=0.125, C=1.0, class_weight='balanced')\n",
    "cl.fit(features, label)\n",
    "\n",
    "sc = cl.score(X_test, y_test)\n",
    "print('Kết quả train model, độ chính xác = ', sc*100, '%')\n",
    "\n",
    "# Save model\n",
    "dump(cl, 'model_phoBERT.pkl')\n",
    "print(\"Đã lưu model SVM vào file model_phoBERT.pkl.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test dataset: 94.99%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.95      0.95      3037\n",
      "    positive       0.95      0.95      0.95      3066\n",
      "\n",
      "    accuracy                           0.95      6103\n",
      "   macro avg       0.95      0.95      0.95      6103\n",
      "weighted avg       0.95      0.95      0.95      6103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from joblib import load\n",
    "# Load model\n",
    "loaded_model = load('model_phoBERT.pkl')\n",
    "# Chia tập dữ liệu\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=86)\n",
    "\n",
    "# Predict labels for the test dataset using the loaded model\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy of the model on the test dataset: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "report = classification_report(y_test, y_pred, output_dict=False)  # Use output_dict=True to get the report as a dictionary\n",
    "print('Classification Report:\\n', report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán: ['positive']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from underthesea import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from joblib import dump\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import regex\n",
    "from joblib import load\n",
    "\n",
    "# Load mô hình SVM\n",
    "cl = load('model_phoBERT.pkl')\n",
    "\n",
    "# Load PhoBERT và tokenizer\n",
    "phobert, tokenizer = load_bert()  # Giả sử hàm load_bert() đã được định nghĩa như trước\n",
    "\n",
    "# Hàm tiền xử lý câu văn mới\n",
    "def preprocess_text(text, tokenizer, model, stopwords, max_len=20):\n",
    "    text = standardize_data(text)  # Giả sử hàm này đã được định nghĩa\n",
    "    text = \" \".join([word for word in word_tokenize(text) if word.lower() not in stopwords])\n",
    "    encoded = tokenizer.encode_plus(text, add_special_tokens=True, max_length=max_len, return_tensors=\"pt\", padding='max_length', return_attention_mask=True, truncation=True)\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    features = last_hidden_states[0][:, 0, :].detach().numpy()\n",
    "    return features\n",
    "\n",
    "# Load danh sách stopwords\n",
    "stopwords = load_stopwords()  # Giả sử hàm này đã được định nghĩa\n",
    "\n",
    "# Tiền xử lý và dự đoán cho câu mới\n",
    "new_sentence = \"hàng không đẹp\"\n",
    "features_new = preprocess_text(new_sentence, tokenizer, phobert, stopwords)\n",
    "prediction = cl.predict(features_new)\n",
    "\n",
    "print(\"Dự đoán:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from underthesea import word_tokenize\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from joblib import dump, load\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Đọc dữ liệu\n",
    "data = pd.read_csv(r\"D:\\projects\\Cap1\\phoBERT\\phoBERT\\TXL_Smarphone.csv\")\n",
    "data['content_cmt'] = data['content_cmt'].astype(str)\n",
    "\n",
    "# Tải mô hình SVM và PhoBERT\n",
    "cl = load('model_phoBERT.pkl')\n",
    "phobert, tokenizer = load_bert()\n",
    "\n",
    "# Hàm tiền xử lý và dự đoán\n",
    "def preprocess_and_predict(text, tokenizer, model, stopwords, svm_model, max_len=20):\n",
    "    if pd.isna(text):\n",
    "        return None\n",
    "    text = standardize_data(text)\n",
    "    text = \" \".join([word for word in word_tokenize(text) if word.lower() not in stopwords])\n",
    "    encoded = tokenizer.encode_plus(\n",
    "        text, add_special_tokens=True, max_length=max_len,\n",
    "        return_tensors=\"pt\", padding='max_length',\n",
    "        return_attention_mask=True, truncation=True\n",
    "    )\n",
    "    input_ids = encoded['input_ids']\n",
    "    attention_mask = encoded['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    features = last_hidden_states[0][:, 0, :].detach().numpy()\n",
    "    return svm_model.predict(features)[0]\n",
    "\n",
    "# Tải danh sách stopwords\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# Dự đoán cảm xúc cho từng bình luận\n",
    "data['sentiment_comment'] = data['content_cmt'].apply(lambda x: preprocess_and_predict(x, tokenizer, phobert, stopwords, cl))\n",
    "\n",
    "# Lưu kết quả\n",
    "data.to_csv(r\"D:\\projects\\Cap1\\phoBERT\\phoBERT\\TXL_Smarphone.csv\", index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
